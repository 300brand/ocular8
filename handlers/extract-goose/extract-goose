#!/usr/bin/env python2
import argparse
import bson
import dateutil.parser
import goose
import logging
import lxml
import pymongo
import urllib

logging.basicConfig(format='%(levelname).1s%(asctime)s.%(msecs)06d %(process)7d %(filename)s:%(lineno)d] %(message)s', datefmt='%m%d %H:%M:%S', level=logging.DEBUG)

def process(db, article, prefix):
	if 'html' not in article or article['html'] is None or len(article['html']) == 0:
		logging.error('%s No HTML', prefix)
		return False

	g = goose.Goose({
		'enable_image_fetching':False
	})
	a = g.extract(raw_html=article['html'])

	if a.cleaned_text != '':
		logging.debug('%s Goose Body %d', prefix, len(a.cleaned_text))

	bodyhtml = ''
	bodyxpath = ''
	if a.top_node is not None:
		bodyhtml = lxml.etree.tostring(a.top_node, pretty_print=False)
		bodyxpath = a.doc.getroottree().getpath(a.top_node)

	update = {
		'$set': {
			'bodyhtml': bodyhtml,
			'bodytext': a.cleaned_text,
			'goose': {
				'bodyxpath': bodyxpath,
				'authors': a.authors,
				'published': a.publish_date,
				'title': a.title
			}
		}
	}

	if a.title != '':
		logging.debug('%s Goose Title', prefix)
		update['$set']['title'] = a.title

	if len(a.authors) > 0:
		logging.debug('%s Goose Author', prefix)
		update['$set']['author'] = a.authors[0]

	if a.publish_date is not None:
		logging.debug('%s Goose Published', prefix)
		update['$set']['published'] = dateutil.parser.parse(a.publish_date)

	db.articles.update({ '_id': article['_id'] }, update, multi=False)

	return True

def main():
	parser = argparse.ArgumentParser(description='Extract meta info and content from articles using Goose')
	parser.add_argument('-mongo', help='MongoDB connection string', default='mongodb://localhost:27017/ocular8')
	parser.add_argument('-nsqdhttp', help='NSQd HTTP address', default='http://localhost:4151')
	parser.add_argument('id', help='article IDs', nargs='+')
	args = parser.parse_args()

	db = pymongo.MongoClient(args.mongo).get_default_database()
	bson_ids = [ bson.objectid.ObjectId(id) for id in args.id ]
	for id in bson_ids:
		article = db.articles.find_one(id)
		if article is None:
			logging.error('%s Not found', id)
			continue

		prefix = 'P:%s F:%s A:%s' % (article['pubid'], article['feedid'], article['_id'])
		if not process(db, article, prefix):
			continue

		topic = 'article.id.elastic'

		q = {
			'_id': article['pubid'],
			'$or': [
				{ 'xpathauthor': { '$exists': True, '$not': { '$size': 0 } } },
				{ 'xpathbody':   { '$exists': True, '$not': { '$size': 0 } } },
				{ 'xpathdate':   { '$exists': True, '$not': { '$size': 0 } } },
				{ 'xpathtitle':  { '$exists': True, '$not': { '$size': 0 } } }
			]
		}
		pub = db.pubs.find_one(q)
		if pub is not None:
			topic = 'article.id.extract.xpath'

		payload = str(article['_id']).encode('utf-8')
		nsqurl = '%s/pub?topic=%s' % (args.nsqdhttp, topic)
		try:
			urllib.urlopen(nsqurl, data=payload)
		except Exception as err:
			logging.error('%s Error while sending data to %s: %s', prefix, nsqurl, err)
		else:
			logging.info('%s Sent to %s', prefix, topic)


if __name__ == '__main__':
	main()
