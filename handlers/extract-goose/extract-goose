#!/usr/bin/env python2
import argparse
import bson
import dateutil.parser
import goose
import lxml
import pymongo
import urllib

def process(db, article):
	if 'html' not in article or article['html'] is None or len(article['html']) == 0:
		print article['_id'], "NO HTML"
		return False

	g = goose.Goose({
		'enable_image_fetching':False
	})
	a = g.extract(raw_html=article['html'])

	bodyhtml = ''
	if a.top_node is not None:
		bodyhtml = lxml.etree.tostring(a.top_node, pretty_print=False)

	update = {
		'$set': {
			'bodyhtml': bodyhtml,
			'bodytext': a.cleaned_text,
			'goose': {
				'authors': a.authors,
				'published': a.publish_date,
				'title': a.title
			}
		}
	}

	if a.title != '':
		update['$set']['title'] = a.title

	if len(a.authors) > 0:
		update['$set']['author'] = a.authors[0]

	if a.publish_date is not None:
		update['$set']['published'] = dateutil.parser.parse(a.publish_date)

	db.articles.update({ '_id': article['_id'] }, update, multi=False)

	return True

def main():
	parser = argparse.ArgumentParser(description='Download articles and push into queue for processing')
	parser.add_argument('-mongo', help='MongoDB connection string', default='mongodb://localhost:27017/ocular8')
	parser.add_argument('-nsqdhttp', help='NSQd HTTP address', default='http://localhost:4151')
	parser.add_argument('id', help='feed_article IDs', nargs='+')
	args = parser.parse_args()

	db = pymongo.MongoClient(args.mongo).get_default_database()
	bson_ids = [ bson.objectid.ObjectId(id) for id in args.id ]
	for id in bson_ids:
		try:
			article = db.articles.find_one(id)
			if not process(db, article):
				continue

			payload = str(article['_id']).encode('utf-8')
			topic = 'article.id.elastic'

			q = {
				'_id': article['pubid'],
				'$or': [
					{ 'xpathauthor': { '$not': { '$size': 0 } } },
					{ 'xpathbody':   { '$not': { '$size': 0 } } },
					{ 'xpathdate':   { '$not': { '$size': 0 } } },
					{ 'xpathtitle':  { '$not': { '$size': 0 } } }
				]
			}
			print q
			pub = db.pubs.find_one(q)
			if pub is not None:
				topic = 'article.id.extract.xpath'

			urllib.urlopen("%s/pub?topic=%s" % (args.nsqdhttp, topic), data=payload)
		except Exception as err:
			print id, str(err)

if __name__ == '__main__':
	main()
