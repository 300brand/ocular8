#!/usr/bin/env python3
import argparse
import bson
import dateutil.parser
import logging
import lxml.etree
import pymongo
import urllib.request

def process(db, lexisnexis):

def main():
	parser = argparse.ArgumentParser(description='Extract meta info and content from articles using XPath')
	parser.add_argument('-mongo', help='MongoDB connection string', default='mongodb://localhost:27017/ocular8')
	parser.add_argument('-nsqdhttp', help='NSQd HTTP address', default='http://localhost:4151')
	parser.add_argument('id', help='LexisNexis IDs', nargs='+')
	args = parser.parse_args()

	db = pymongo.MongoClient(args.mongo).get_default_database()
	bson_ids = [ bson.objectid.ObjectId(id) for id in args.id ]
	for id in bson_ids:
		lexisnexis = db.lexisnexis.find_one(id)
		if lexisnexis is None:
			logging.error('%s Article not found', id)
			continue

		if not process(db, lexisnexis):
			continue

		payload = str(lexisnexis['_id']).encode('utf-8')
		topic = 'article.id.extract.goose'
		nsqurl = '%s/pub?topic=%s' % (args.nsqdhttp, topic)
		try:
			urllib.request.urlopen(nsqurl, data=payload)
		except Exception as err:
			logging.error('%s Error while sending data to %s: %s', prefix, nsqurl, err)
		else:
			logging.info('%s Sent to %s', prefix, topic)

if __name__ == "__main__":
	main()
