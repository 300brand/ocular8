#!/usr/bin/env python3
import argparse
import bson
import datetime
import etcd
import feedparser
import logging
import pymongo
import time
import urllib.request

logging.basicConfig(format='%(levelname).1s%(asctime)s.%(msecs)06d %(process)7d %(filename)s:%(lineno)d] %(message)s', datefmt='%m%d %H:%M:%S', level=logging.DEBUG)

def process(db, feed, nsq):
	prefix = 'P:%s F:%s' % (feed['pubid'], feed['_id'])

	etag = feed.get('etag', None)
	modified = feed.get('lastmodified', None)
	doc = feedparser.parse(feed['url'], etag=etag, modified=modified)

	status = doc.get('status', 500)

	logging.info('%s STATUS %d', prefix, status)

	nextdownload = None
	if doc['bozo'] == 0:
		nextdownload = datetime.datetime.utcnow() + datetime.timedelta(hours=1)

	sets = {
		'bozo':         doc['bozo'],
		'etag':         doc.get('etag', None),
		'lastmodified': doc.get('modified', None),
		'laststatus':   status,
		'lastdownload': datetime.datetime.utcnow(),
		'nextdownload': nextdownload
	}
	if 'updated_parsed' in doc and doc['updated_parsed'] is not None:
		t = datetime.datetime.utcfromtimestamp(time.mktime(doc['updated_parsed']))
		sets['updated'] = t
	if status == 301:
		logging.info('%s Updating feed URL from %s to %s', prefix, feed['url'], doc['href'])
		sets['originalurl'] = feed['url']
		sets['url'] = doc['href']

	update = {
		'$set': sets,
		'$inc': {
			'downloads': 1
		}
	}

	db.feeds.update({ '_id': feed['_id'] }, update)

	if status in [ 304, 410, 500 ]:
		logging.warning('%s Bailing from bad status: %d', prefix, status)
		return

	ids = []
	for entry in doc.entries:
		if 'link' not in entry:
			logging.error('%s No link field in entry', prefix)
			continue

		if db.entries.find_one({ 'link': entry['link'] }) is not None:
			logging.warning('%s URL not unique: %s', prefix, entry['link'])
			continue

		entry['_id'] = bson.objectid.ObjectId()
		entry['feedid'] = feed['_id']
		entry['pubid'] = feed['pubid']
		logging.info('%s E:%s New entry', prefix, entry['_id'])

		db.entries.insert(entry)
		ids.append(str(entry['_id']))

	payload = ('\n'.join(ids) + '\n').encode('utf-8')
	topic = "entry.id.download"
	nsqurl = "%s/mpub?topic=%s" % (nsq, topic)
	try:
		urllib.request.urlopen(nsqurl, data=payload)
	except Exception as err:
		logging.error('%s Error while sending data to %s: %s', prefix, nsqurl, err)
	else:
		logging.info('%s Sent %d new entries to %s', prefix, len(ids), topic)

def main():
	parser = argparse.ArgumentParser(description='Download feed and push new URLs into next queue')
	parser.add_argument('-etcd', help='Etcd URL', default='http://localhost:4001')
	parser.add_argument('id', help='Feed ObjectId', nargs='+')
	args = parser.parse_args()

	url = urllib.parse.urlsplit(args.etcd)
	client = etcd.Client(host=url.hostname, port=url.port)
	config_mongo = client.get('/config/mongo').value
	config_nsqhttp = client.get('/config/nsqhttp').value

	db = pymongo.MongoClient(config_mongo).get_default_database()

	bson_ids = [ bson.objectid.ObjectId(id) for id in args.id ]
	feeds = db.feeds.find({ '_id': { '$in': bson_ids } })
	for feed in feeds:
		process(db, feed, config_nsqhttp)

if __name__ == '__main__':
	main()
