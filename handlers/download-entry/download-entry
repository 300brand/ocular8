#!/usr/bin/env python3
import argparse
import bson
import cleanurl
import datetime
import logging
import pymongo
import time
import urllib.error
import urllib.request

logging.basicConfig(format='%(levelname).1s%(asctime)s.%(msecs)06d %(process)7d %(filename)s:%(lineno)d] %(message)s', datefmt='%m%d %H:%M:%S', level=logging.DEBUG)

def process(db, entry, nsq):
	prefix = 'P:%s F:%s E:%s' % (entry['pubid'], entry['feedid'], entry['_id'])

	start = datetime.datetime.now()
	try:
		logging.debug('%s Entry Link: %s', prefix, entry['link'])
		url, req = cleanurl.clean(entry['link'])
		logging.debug('%s Clean URL:  %s', prefix, url)
	except urllib.error.HTTPError as err:
		logging.error('%s HTTPError: %d %s - %s', prefix, err.code, err.reason, entry['link'])
		db.article_errors.insert({
			'url':    entry['link'],
			'type':   'HTTPError',
			'code':   err.code,
			'reason': err.reason,
			'entry':  entry
		})
		db.entries.remove(entry['_id'], multi=False)
		return
	except urllib.error.URLError as err:
		logging.error('%s URLError: %d %s - %s', prefix, err.reason[0], err.reason[1], entry['link'])
		db.article_errors.insert({
			'url':    entry['link'],
			'type':   'URLError',
			'code':   err.reason[0],
			'reason': err.reason[1],
			'entry':  entry
		})
		db.entries.remove(entry['_id'], multi=False)
		return

	if db.articles.find_one({ 'url': url }) is not None:
		logging.warning('%s Not unique: %s', prefix, url)
		return

	html = req.read()

	published = entry.get('published_parsed', None)
	if published is not None:
		published = datetime.datetime(*published[:6])

	dur = datetime.datetime.now() - start
	loadtime = (dur.microseconds + dur.seconds * 10**6) * 1000

	article = {
		'_id':       entry['_id'],
		'feedid':    entry['feedid'],
		'pubid':     entry['pubid'],
		'author':    entry.get('author', None),
		'published': published,
		'title':     entry.get('title', None),
		'url':       url,
		'html':      html,
		'loadtime':  loadtime,
		'entry':     {
			'author':    entry.get('author', None),
			'published': published,
			'title':     entry.get('title', None),
			'url':       entry['link']
		}
	}
	
	logging.info('%s Moving to articles collection', prefix)
	db.articles.insert(article)
	db.entries.remove(entry['_id'], multi=False)

	payload = str(entry['_id']).encode('utf-8')
	topic = 'article.id.extract.goose'
	nsqurl = '%s/pub?topic=%s' % (nsq, topic)
	try:
		urllib.request.urlopen(nsqurl, data=payload)
	except Exception as err:
		logging.error('%s Error while sending data to %s: %s', prefix, nsqurl, err)
	else:
		logging.info('%s Sent to %s', prefix, topic)

def main():
	parser = argparse.ArgumentParser(description='Download articles and push into queue for processing')
	parser.add_argument('-mongo', help='MongoDB connection string', default='mongodb://localhost:27017/ocular8')
	parser.add_argument('-nsqdhttp', help='NSQd HTTP address', default='http://localhost:4151')
	parser.add_argument('id', help='feed_article IDs', nargs='+')
	args = parser.parse_args()

	db = pymongo.MongoClient(args.mongo).get_default_database()
	bson_ids = [ bson.objectid.ObjectId(id) for id in args.id ]
	for id in bson_ids:
		entry = db.entries.find_and_modify({ '_id': id }, update={ '$inc': { 'attempts': 1 } })
		if entry is None:
			logging.error('%s Not found', id)
			continue
		process(db, entry, args.nsqdhttp)

if __name__ == "__main__":
	main()
