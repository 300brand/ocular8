#!/usr/bin/env python3
import argparse
import bson
import dateutil.parser
import elasticsearch
import etcd
import json
import logging
import lxml.etree
import mysql.connector
import urllib.parse
import urllib.request


STMT_SELECT = "SELECT data FROM processing WHERE article_id = %(article_id)s LIMIT 1"
STMT_UPDATE = "UPDATE processing SET data = %(data)s WHERE article_id = %(article_id)s LIMIT 1"

logging.basicConfig(format='%(levelname).1s%(asctime)s.%(msecs)06d %(process)7d %(filename)s:%(lineno)d] %(message)s', datefmt='%m%d %H:%M:%S', level=logging.WARNING)

def process(db, es, index, article_id):
	cursor = db.cursor()
	cursor.execute(STMT_SELECT, { 'article_id': article_id })
	row = cursor.fetchone()
	article = json.loads(row[0].decode('utf-8'))
	prefix = 'P:%s F:%s A:%s' % (article['PubId'], article['FeedId'], article['ArticleId'])

	if 'HTML' not in article or article['HTML'] is None or len(article['HTML']) == 0:
		logging.error('%s No HTML', prefix)
		return False

	rawpubpub = es.get(index=index, doc_type='pub', id=article['PubId'])
	if rawpubpub is None:
		logger.error('No pub found for %s', article['PubId'])
		return

	pub = rawpubpub['_source']
	extracted = []

	if 'XPathAuthor' in pub:
		v = extract(article, pub['XPathAuthor'])
		logging.debug('%s XPath Author: %r', prefix, v is not None)
		if v is not None:
			extracted.append(('Author', v))

	if ('BodyText' not in article or len(article['BodyText']) == 0) and 'XPathBody' in pub:
		v = extract(article, pub['XPathBody'])
		logging.debug('%s XPath Body: %r', prefix, v is not None)
		if v is not None:
			extracted.append(('BodyText', v))

	if 'XPathDate' in pub:
		v = extract(article, pub['XPathDate'])
		logging.debug('%s XPath Published: %r', prefix, v is not None)
		if v is not None:
			try:
				parsed = dateutil.parser.parse(v)
				v = str(parsed).replace(' ', 'T')
				if parsed.strftime('%z') == '':
					v += 'Z'
			except ValueError as err:
				logging.warning('%s Could not dateutil.parser.parse "%s"', prefix, v)
			except Exception as err:
				logging.error('%s Unexpected Exception: %s', prefix, err)
			extracted.append(('Published', v))

	if 'XPathTitle' in pub:
		v = extract(article, pub['XPathTitle'])
		logging.debug('%s XPath Title: %r', prefix, v is not None)
		if v is not None:
			extracted.append(('Title', v))

	# Re-apply extracted contents to article object
	article['XPath'] = {}
	for e in extracted:
		(k, v) = e
		article[k] = v
		if k == 'BodyText':
			# Squash BodyHTML if BodyText is found, may need to go back and
			# include the DOM element in extract's return
			article['BodyHTML'] = ''
			continue
		article['XPath'][k] = v

	cursor.execute(STMT_UPDATE, { 'article_id': article_id, 'data': json.dumps(article) })

	db.commit()
	cursor.close()

	return True

def extract(article, xpaths):
	if len(xpaths) == 0:
		return None

	parser = lxml.etree.HTMLParser()
	tree = lxml.etree.fromstring(article['HTML'], parser)
	remove = []
	search = []
	for xpath in xpaths:
		if xpath[0] == '-':
			remove.append(xpath[1:])
		else:
			if '~~' in xpath:
				# Trim off date format from legacy XPaths
				xpath = xpath[:xpath.find('~~')]
			search.append(xpath)

	for r in remove:
		for child in tree.xpath(r):
			child.getparent().remove(child)

	for s in search:
		if not s.startswith('normalize-space'):
			s = 'normalize-space(' + s + ')'

		found = tree.xpath(s)
		if len(found) == 0:
			continue

		return found

	return None

def nsq_id(nsqhttp, topic, id):
	payload = id.encode('utf-8')
	nsqurl = '%s/pub?topic=%s' % (nsqhttp, topic)
	try:
		urllib.request.urlopen(nsqurl, data=payload)
	except Exception as err:
		logging.error('Error while sending data to %s: %s', nsqurl, err)
	else:
		logging.info('Sent to %s', topic)

def main():
	parser = argparse.ArgumentParser(description='Extract meta info and content from articles using XPath')
	parser.add_argument('-etcd', help='Etcd URL', default='http://localhost:4001')
	parser.add_argument('id', help='Article ObjectId', nargs='+')
	args = parser.parse_args()

	url = urllib.parse.urlsplit(args.etcd)
	client = etcd.Client(host=url.hostname, port=url.port)
	config_elastic = client.get('/config/elastichosts').value.split(",")
	config_index   = client.get('/config/elasticindex').value
	config_mysql   = json.loads(client.get('/config/mysqlobj').value)
	config_nsqhttp = client.get('/config/nsqhttp').value
	config_topic   = client.get('/handlers/elastic-save/consume').value

	es = elasticsearch.Elasticsearch(hosts=config_elastic)
	db = mysql.connector.connect(**config_mysql)

	for id in args.id:
		if not process(db, es, config_index, id):
			continue
		nsq_id(config_nsqhttp, config_topic, id)

	db.close()

if __name__ == "__main__":
	main()
