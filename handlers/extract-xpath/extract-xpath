#!/usr/bin/env python3
import argparse
import bson
import dateutil.parser
import lxml.etree
import pymongo
import urllib.request

def process(db, pub, article):
	if 'html' not in article or article['html'] is None or len(article['html']) == 0:
		print(article['_id'], 'NO HTML')
		return False

	extracted = []

	if len(pub['xpathauthor']) > 0:
		extracted.append(('author', extract(article, pub['xpathauthor'])))

	if ('bodytext' not in article or article['bodytext'] is None or article['bodytext'] == '') and len(pub['xpathbody']) > 0:
		extracted.append(('bodytext', extract(article, pub['xpathbody'])))

	if len(pub['xpathdate']) > 0:
		v = extract(article, pub['xpathdate'])
		if v is not None:
			v = dateutil.parser.parse(v)
		extracted.append(('published', v))

	if len(pub['xpathtitle']) > 0:
		extracted.append(('title', extract(article, pub['xpathtitle'])))

	update = {
		'$set': {
			'xpath': {}
		}
	}

	for e in extracted:
		(k, v) = e
		update['$set']['xpath'][k] = v
		update['$set'][k] = v

	db.articles.update({'_id': article['_id']}, update)

	return True

def extract(article, xpaths):
	parser = lxml.etree.HTMLParser()
	tree = lxml.etree.fromstring(article['html'], parser)
	remove = []
	search = []
	for xpath in xpaths:
		if xpath[0] == '-':
			remove.append(xpath[1:])
		else:
			if '~~' in xpath:
				xpath = xpath[:xpath.find('~~')]
			search.append(xpath)

	for r in remove:
		for child in tree.xpath(r):
			child.getparent().remove(child)

	for s in search:
		if not s.startswith('normalize-space'):
			s = 'normalize-space(' + s + ')'

		found = tree.xpath(s)
		if len(found) == 0:
			print(article['_id'], 'NOTHING FOUND', s)
			continue

		return found

	return None


def main():
	parser = argparse.ArgumentParser(description='Extract meta info and content from articles using XPath')
	parser.add_argument('-mongo', help='MongoDB connection string', default='mongodb://localhost:27017/ocular8')
	parser.add_argument('-nsqdhttp', help='NSQd HTTP address', default='http://localhost:4151')
	parser.add_argument('id', help='article IDs', nargs='+')
	args = parser.parse_args()

	db = pymongo.MongoClient(args.mongo).get_default_database()
	bson_ids = [ bson.objectid.ObjectId(id) for id in args.id ]
	for id in bson_ids:
		try:
			article = db.articles.find_one(id)
			if article is None:
				print(id, 'ARTICLE NOT FOUND')
				continue

			q = {
				'_id': article['pubid'],
				'$or': [
					{ 'xpathauthor': { '$not': { '$size': 0 } } },
					{ 'xpathbody':   { '$not': { '$size': 0 } } },
					{ 'xpathdate':   { '$not': { '$size': 0 } } },
					{ 'xpathtitle':  { '$not': { '$size': 0 } } }
				]
			}
			pub = db.pubs.find_one(q)
			if pub is None:
				print(id, article['pubid'], 'PUB NOT FOUND')
				continue

			if not process(db, pub, article):
				continue

			payload = str(article['_id']).encode('utf-8')
			urllib.request.urlopen("%s/pub?topic=%s" % (args.nsqdhttp, 'article.id.elastic'), data=payload)
		except Exception as err:
			print(id, str(err))

if __name__ == "__main__":
	main()
