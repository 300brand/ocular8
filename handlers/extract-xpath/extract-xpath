#!/usr/bin/env python3
import argparse
import bson
import dateutil.parser
import etcd
import logging
import lxml.etree
import pymongo
import urllib.parse
import urllib.request

logging.basicConfig(format='%(levelname).1s%(asctime)s.%(msecs)06d %(process)7d %(filename)s:%(lineno)d] %(message)s', datefmt='%m%d %H:%M:%S', level=logging.DEBUG)

def process(db, pub, article, prefix):
	if 'html' not in article or article['html'] is None or len(article['html']) == 0:
		logging.error('%s No HTML', prefix)
		return False

	extracted = []

	if len(pub['xpathauthor']) > 0:
		logging.debug('%s XPath Author: %d', prefix, len(pub['xpathauthor']))
		if not extracted.append(('author', extract(article, pub['xpathauthor']))):
			logging.warning('%s Nothing found for Author', prefix)

	if ('bodytext' not in article or article['bodytext'] is None or article['bodytext'] == '') and len(pub['xpathbody']) > 0:
		logging.debug('%s XPath Body: %d', prefix, len(pub['xpathbody']))
		if not extracted.append(('bodytext', extract(article, pub['xpathbody']))):
			logging.warning('%s Nothing found for Body', prefix)

	if len(pub['xpathdate']) > 0:
		logging.debug('%s XPath Published: %d', prefix, len(pub['xpathdate']))
		v = extract(article, pub['xpathdate'])
		if v is not None:
			try:
				v = dateutil.parser.parse(v)
			except ValueError as err:
				logging.warning('%s Could not dateutil.parser.parse "%s"', prefix, v)
			except Exception as err:
				logging.error('%s Unexpected Exception: %s', prefix, err)

		if not extracted.append(('published', v)):
			logging.warning('%s Nothing found for Published', prefix)

	if len(pub['xpathtitle']) > 0:
		logging.debug('%s XPath Title: %d', prefix, len(pub['xpathtitle']))
		if not extracted.append(('title', extract(article, pub['xpathtitle']))):
			logging.warning('%s Nothing found for Title', prefix)

	update = {
		'$set': {
			'xpath': {}
		}
	}

	for e in extracted:
		(k, v) = e
		update['$set']['xpath'][k] = v
		update['$set'][k] = v

	db.articles.update({'_id': article['_id']}, update)

	return True

def extract(article, xpaths):
	parser = lxml.etree.HTMLParser()
	tree = lxml.etree.fromstring(article['html'], parser)
	remove = []
	search = []
	for xpath in xpaths:
		if xpath[0] == '-':
			remove.append(xpath[1:])
		else:
			if '~~' in xpath:
				# Trim off date format from legacy XPaths
				xpath = xpath[:xpath.find('~~')]
			search.append(xpath)

	for r in remove:
		for child in tree.xpath(r):
			child.getparent().remove(child)

	for s in search:
		if not s.startswith('normalize-space'):
			s = 'normalize-space(' + s + ')'

		found = tree.xpath(s)
		if len(found) == 0:
			continue

		return found

	return None


def main():
	parser = argparse.ArgumentParser(description='Extract meta info and content from articles using XPath')
	parser.add_argument('-etcd', help='Etcd URL', default='http://localhost:4001')
	parser.add_argument('id', help='Article ObjectId', nargs='+')
	args = parser.parse_args()

	url = urllib.parse.urlsplit(args.etcd)
	client = etcd.Client(host=url.hostname, port=url.port)
	config_mongo = client.get('/config/mongo').value
	config_nsqhttp = client.get('/config/nsqhttp').value

	db = pymongo.MongoClient(config_mongo).get_default_database()
	bson_ids = [ bson.objectid.ObjectId(id) for id in args.id ]
	for id in bson_ids:
		article = db.articles.find_one(id)
		if article is None:
			logging.error('%s Article not found', id)
			continue

		prefix = 'P:%s F:%s A:%s' % (article['pubid'], article['feedid'], article['_id'])

		q = {
			'_id': article['pubid'],
			'$or': [
				{ 'xpathauthor': { '$not': { '$size': 0 } } },
				{ 'xpathbody':   { '$not': { '$size': 0 } } },
				{ 'xpathdate':   { '$not': { '$size': 0 } } },
				{ 'xpathtitle':  { '$not': { '$size': 0 } } }
			]
		}
		pub = db.pubs.find_one(q)
		if pub is None:
			logging.error('%s Pub not found', prefix)
			continue

		if not process(db, pub, article, prefix):
			continue

		payload = str(article['_id']).encode('utf-8')
		topic = 'article.id.elastic'
		nsqurl = '%s/pub?topic=%s' % (config_nsqhttp, topic)
		try:
			urllib.request.urlopen(nsqurl, data=payload)
		except Exception as err:
			logging.error('%s Error while sending data to %s: %s', prefix, nsqurl, err)
		else:
			logging.info('%s Sent to %s', prefix, topic)

if __name__ == "__main__":
	main()
